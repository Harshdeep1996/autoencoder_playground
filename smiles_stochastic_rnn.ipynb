{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load /mnt/c/Users/harshdeep.harshdeep/AppData/Local/Packages/CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc/LocalState/stochastic_rnn.py\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "\n",
    "class ConditionalNormalDistribution(object):\n",
    "  \"\"\"A Normal distribution conditioned on Tensor inputs via a fc network.\"\"\"\n",
    "\n",
    "  def __init__(self, size, hidden_layer_sizes, sigma_min=0.0,\n",
    "               raw_sigma_bias=0.25, hidden_activation_fn=tf.nn.relu,\n",
    "               initializers=None, name=\"conditional_normal_distribution\"):\n",
    "    \"\"\"Creates a conditional Normal distribution.\n",
    "    Args:\n",
    "      size: The dimension of the random variable.\n",
    "      hidden_layer_sizes: The sizes of the hidden layers of the fully connected\n",
    "        network used to condition the distribution on the inputs.\n",
    "      sigma_min: The minimum standard deviation allowed, a scalar.\n",
    "      raw_sigma_bias: A scalar that is added to the raw standard deviation\n",
    "        output from the fully connected network. Set to 0.25 by default to\n",
    "        prevent standard deviations close to 0.\n",
    "      hidden_activation_fn: The activation function to use on the hidden layers\n",
    "        of the fully connected network.\n",
    "      initializers: The variable intitializers to use for the fully connected\n",
    "        network. The network is implemented using snt.nets.MLP so it must\n",
    "        be a dictionary mapping the keys 'w' and 'b' to the initializers for\n",
    "        the weights and biases. Defaults to xavier for the weights and zeros\n",
    "        for the biases when initializers is None.\n",
    "      name: The name of this distribution, used for sonnet scoping.\n",
    "    \"\"\"\n",
    "    self.sigma_min = sigma_min\n",
    "    self.raw_sigma_bias = raw_sigma_bias\n",
    "    self.name = name\n",
    "    self.size = size\n",
    "    if initializers is None:\n",
    "      initializers = DEFAULT_INITIALIZERS\n",
    "    self.fcnet = snet.nets.MLP(\n",
    "        output_sizes=hidden_layer_sizes + [2 * size],\n",
    "        activation=hidden_activation_fn,\n",
    "        initializers=initializers,\n",
    "        activate_final=False,\n",
    "        use_bias=True,\n",
    "        name=name + \"_fcnet\")\n",
    "\n",
    "  def condition(self, tensor_list, **unused_kwargs):\n",
    "    \"\"\"Computes the parameters of a normal distribution based on the inputs.\"\"\"\n",
    "    inputs = tf.concat(tensor_list, axis=1)\n",
    "    outs = self.fcnet(inputs)\n",
    "    mu, sigma = tf.split(outs, 2, axis=1)\n",
    "    sigma = tf.maximum(tf.nn.softplus(sigma + self.raw_sigma_bias),\n",
    "                       self.sigma_min)\n",
    "    return mu, sigma\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    \"\"\"Creates a normal distribution conditioned on the inputs.\"\"\"\n",
    "    mu, sigma = self.condition(args, **kwargs)\n",
    "    return tf.contrib.distributions.Normal(loc=mu, scale=sigma)\n",
    "\n",
    "\n",
    "class NormalApproximatePosterior(ConditionalNormalDistribution):\n",
    "  \"\"\"A Normally-distributed approx. posterior with res_q parameterization.\"\"\"\n",
    "\n",
    "  def __init__(self, size, hidden_layer_sizes, sigma_min=0.0,\n",
    "               raw_sigma_bias=0.25, hidden_activation_fn=tf.nn.relu,\n",
    "               initializers=None, smoothing=False,\n",
    "               name=\"conditional_normal_distribution\"):\n",
    "    super(NormalApproximatePosterior, self).__init__(\n",
    "        size, hidden_layer_sizes, sigma_min=sigma_min,\n",
    "        raw_sigma_bias=raw_sigma_bias,\n",
    "        hidden_activation_fn=hidden_activation_fn, initializers=initializers,\n",
    "        name=name)\n",
    "    self.smoothing = smoothing\n",
    "\n",
    "  def condition(self, tensor_list, prior_mu, smoothing_tensors=None):\n",
    "    \"\"\"Generates the mean and variance of the normal distribution.\n",
    "    Args:\n",
    "      tensor_list: The list of Tensors to condition on. Will be concatenated and\n",
    "        fed through a fully connected network.\n",
    "      prior_mu: The mean of the prior distribution associated with this\n",
    "        approximate posterior. Will be added to the mean produced by\n",
    "        this approximate posterior, in res_q fashion.\n",
    "      smoothing_tensors: A list of Tensors. If smoothing is True, these Tensors\n",
    "        will be concatenated with the tensors in tensor_list.\n",
    "    Returns:\n",
    "      mu: The mean of the approximate posterior.\n",
    "      sigma: The standard deviation of the approximate posterior.\n",
    "    \"\"\"\n",
    "    if self.smoothing:\n",
    "      tensor_list.extend(smoothing_tensors)\n",
    "    mu, sigma = super(NormalApproximatePosterior, self).condition(tensor_list)\n",
    "    return mu + prior_mu, sigma\n",
    "\n",
    "\n",
    "##### STOCHASTIC RNN ######\n",
    "\n",
    "StochasticRNNState = namedtuple('StochasticRNNState', 'rnn_state latent_encoded')\n",
    "TrainableStochasticRNNState = namedtuple('TrainableStochasticRNNState', StochasticRNNState._fields + ('rnn_out', ))\n",
    "\n",
    "class StochasticRNN(object):\n",
    "\n",
    "    def __init__(\n",
    "        self, rnn_cell, data_encoder, transition,\n",
    "        emission, latent_encoder, random_seed=None):\n",
    "        self.rnn_cell = rnn_cell\n",
    "        self.state_size = self.rnn_cell.state_size\n",
    "        self.data_encoder = data_encoder\n",
    "        self.transition_state = transition ## z_t <- z_t-1, h_t ## Normal\n",
    "        self.emission_state = emission ## X_t <- z_t, h_t ## Logprob\n",
    "        # Encoding z into something favorable\n",
    "        self.latent_encoder = latent_encoder\n",
    "        # Dimensions of z\n",
    "        self.encoded_z_side = latent_encoder.output_size\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        ## Make the initial state of the hidden and latent layer - (h and z)\n",
    "        return StochasticRNNState(\n",
    "            rnn_state=self.rnn_cell.zero_state(batch_size, dtype=dtype),\n",
    "            latent_encoded=tf.zeros([batch_size, self.latent_encoder.output_size], dtype=dtype)\n",
    "        )\n",
    "\n",
    "    def run_rnn(self, previous_state, inputs):\n",
    "        # Encode the data x_t into something favorable which could be fed to an RNN\n",
    "        # Function to get the new output and hidden state\n",
    "        rnn_inputs = self.data_encoder(tf.to_float(inputs))\n",
    "        rnn_out, rnn_state = self.rnn_cell(rnn_inputs, previous_state)\n",
    "        return rnn_out, rnn_state # (h,c)\n",
    "\n",
    "    def transition(self, previous_latent, current_hidden_state):\n",
    "        return self.transition_state(current_hidden_state, previous_latent)\n",
    "\n",
    "    def emission(self, current_latent_state, current_hidden_state):\n",
    "        # Encoding the input of the variational autoencoder to the RNN\n",
    "        latent_inputs = self.latent_encoder(tf.to_float(current_latent_state))\n",
    "        return (\n",
    "            self.emission_state(current_hidden_state, latent_inputs),\n",
    "            latent_inputs\n",
    "        )\n",
    "\n",
    "    def sample_step(self, previous_state, inputs, unused_t):\n",
    "        \"\"\"\n",
    "        args: previous_state: this is the previous rnn state and previous encoded latent.\n",
    "        args: input: this is X_t-1 with [batch_size, data_size]\n",
    "        args: the current time step (this can be used for filtering)\n",
    "        \"\"\"\n",
    "        rnn_out, rnn_state = self.run_rnn(previous_state.rnn_state, inputs)\n",
    "        z_t = self.transition(prev_state.latent_encoded, rnn_out)\n",
    "        z_t = z_t.sample(seed=self.random_seed)\n",
    "        x_t, latent_encoded = self.emsission(z_t, rnn_out)\n",
    "        x_t = x_t.sample(seed=self.random_seed)\n",
    "        new_state = StochasticRNNState(rnn_state=rnn_state, latent_encoded=latent_encoded)\n",
    "        return new_state, tf.to_float(x_t)\n",
    "\n",
    "class TrainableStochasticRNN(StochasticRNN):\n",
    "\n",
    "    def __init__(\n",
    "        self, rnn_cell, data_encoder, latent_encoder,\n",
    "        transition, emission, proposal_type, proposal=None,\n",
    "        rev_rnn_cell=None, tilt=None, random_seed=None):\n",
    "\n",
    "        super(TrainableStochasticRNN, self).__init__(\n",
    "            rnn_cell, data_encoder, transition, emission,\n",
    "            latent_encoder, random_seed=random_seed\n",
    "        )\n",
    "        self.rev_rnn_cell = rev_rnn_cell\n",
    "        self._tilt = tilt\n",
    "        ## Callable for proposal to which inputs can be fed\n",
    "        ## hidden state and encoded target of current timestamp\n",
    "        self._proposal = proposal\n",
    "        self.proposal_type = proposal_type\n",
    "\n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        ## Assigning all 3 - input, hidden and latent layer to be zero\n",
    "        super_state = super(TrainableStochasticRNN, self).zero_state(batch_size, dtype)\n",
    "        return TrainableStochasticRNNState(\n",
    "            ## Assigning hidden state to be zeros at the start\n",
    "            rnn_out=tf.zeros([batch_size, self.rnn_cell.output_size], dtype=dtype),\n",
    "            **super_state._asdict() ## Get the params from the StochasticRNNState\n",
    "        )\n",
    "\n",
    "    def ta_for_tensor(self, x, dynamic_size=False, clear_after_read=False):\n",
    "        # Unstack the inputs and outputs based on sequence lengths which need to be fed into the RNN\n",
    "        return tf.TensorArray(\n",
    "            x.dtype, tf.shape(x)[0],\n",
    "            dynamic_size=dynamic_size,\n",
    "            clear_after_read=clear_after_read).unstack(x)\n",
    "\n",
    "    def encode_all(self, inputs, encoder):\n",
    "      \"\"\"Encodes a timeseries of inputs with a time independent encoder.\n",
    "      Args:\n",
    "        inputs: A [time, batch, feature_dimensions] tensor.\n",
    "        encoder: A network that takes a [batch, features_dimensions] input and\n",
    "          encodes the input.\n",
    "      Returns: \n",
    "        A [time, batch, encoded_feature_dimensions] output tensor.\n",
    "      \"\"\"\n",
    "      input_shape = tf.shape(inputs)\n",
    "      num_timesteps, batch_size = input_shape[0], input_shape[1]\n",
    "      reshaped_inputs = tf.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "      inputs_encoded = encoder(reshaped_inputs)\n",
    "      inputs_encoded = tf.reshape(inputs_encoded, [num_timesteps, batch_size, encoder.output_size])\n",
    "      return inputs_encoded\n",
    "\n",
    "    def set_observations(self, observations, seq_lengths):\n",
    "        \"\"\"\n",
    "        args: observations: 2 tensors of shape (max_seq_len, batch_size, data_size)\n",
    "            should be inputs and targets.\n",
    "        args: seq_lengths: tensor of length batch size with all the lengths\n",
    "        \"\"\"\n",
    "        inputs, targets = observations\n",
    "        self.seq_lengths = seq_lengths\n",
    "        self.max_len_seq = tf.reduce_max(seq_lengths)\n",
    "        self.targets_ta = self.ta_for_tensor(targets)\n",
    "        # Encode the targets for the LSTM\n",
    "        targets_encoded = self.encode_all(targets, self.data_encoder)\n",
    "        self.targets_encoded_ta = self.ta_for_tensor(targets_encoded)\n",
    "        # Encode the inputs for the LSTM\n",
    "        inputs_encoded = self.encode_all(inputs, self.data_encoder)\n",
    "\n",
    "        ## Feed it into the RNN\n",
    "        ## time major is letting if the inputs are of the form of [max__l] \n",
    "        ## This is basically getting the next hidden state given the inputs and previous hidden state\n",
    "        rnn_out, _ = tf.nn.dynammic_rnn(\n",
    "            self.rnn_cell, inputs_encoded, time_major=True, dtype=tf.float32, scope='forward_rnn')\n",
    "        self.rnn_ta = self.ta_for_tensor(rnn_out)\n",
    "\n",
    "        # This is for SMOOTHING - learning the inference by reversing the inputs for all time steps\n",
    "        if self.rev_rnn_cell:\n",
    "            targets_and_rnn_out = tf.concat([rnn_out, targets_encoded], 2) # Along 2nd axis/data points\n",
    "            reversed_input = tf.reverse_sequence(targets_and_rnn_out, seq_lengths, seq_axis=0, batch_axis=1)\n",
    "            reverse_rnn_out, _ = tf.nn.dynammic_rnn(\n",
    "                self.rev_rnn_cell, reversed_input, time_major=True, dtype=tf.float32, scope='reverse_rnn'\n",
    "            )\n",
    "            reverse_rnn_out = tf.reverse_sequence(reverse_rnn_out, seq_lengths, seq_axis=0, batch_axis=1)\n",
    "            self.reverse_rnn_ta = self.ta_for_tensor(reverse_rnn_out)\n",
    "\n",
    "    def _filtering_proposal(self, rnn_out, prev_latent_encoded, prior, t):\n",
    "        return self._proposal(\n",
    "            rnn_out, prev_latent_encoded,\n",
    "            self.targets_encoded_ta.read(t), prior_mu=prior.mean())\n",
    "\n",
    "    def _smoothing_proposal(self, rnn_out, prev_latent_encoded, prior, t):\n",
    "        return self._proposal(\n",
    "            rnn_out, prev_latent_encoded,\n",
    "            smoothing_tensors=[self.reverse_rnn_ta.read(t)],\n",
    "            prior_mu=prior.mean())\n",
    "\n",
    "    def proposal(self, rnn_out, prev_latent_encoded, prior, t):\n",
    "        ## Depending upon the proposal call the particular neural network to do\n",
    "        ## an inference for the inputs\n",
    "        if self.proposal_type == 'filtering':\n",
    "            return self._filtering_proposal(rnn_out, prev_latent_encoded, prior, t)\n",
    "        elif self.proposal_type == 'smoothing':\n",
    "            return self._smoothing_proposal(rnn_out, prev_latent_encoded, prior, t)\n",
    "        else:\n",
    "            return self.transition(prev_latent_encoded, rnn_out)\n",
    "\n",
    "    def tilt(self, rnn_out, latent_encoded, targets):\n",
    "        # Calculating the log probability of the output\n",
    "        # given in the hidden state and latent state\n",
    "        r_func = self._tilt(rnn_out, latent_encoded)\n",
    "        return tf.reduce_sum(r_func.log_prob(targets), axis=-1)\n",
    "\n",
    "    def propose_and_weight(self, state, t):\n",
    "        \"\"\"\n",
    "        args: state: the previous state of the model - and an TrainableSRNNState \n",
    "        args: t: the current timestep on which the model is\n",
    "        \"\"\"\n",
    "        ## For running the model and computes importance weights for one timestep\n",
    "        \n",
    "        ## One gets the targets and encoded input for one timestep\n",
    "        targets = self.targets_ta.read(t)\n",
    "        rnn_out = self.rnn_ta.read(t)\n",
    "\n",
    "        # Get the latent space\n",
    "        p_zt = self.transition(state.latent_encoded, rnn_out)\n",
    "        q_zt = self.proposal(rnn_out, state.latent_encoded, p_zt, t)\n",
    "        # Sampling the latent state\n",
    "        z_t = q_zt.sample(random_seed=self.random_seed)\n",
    "\n",
    "        ## Emission phase where we reconstruct the input\n",
    "        p_xt_given_zt, latent_encoded = self.emsission(z_t, rnn_out)\n",
    "        log_p_xt_given_zt = tf.reduce_sum(p_xt_given_zt.log_prob(targets), axis=-1)\n",
    "        log_p_zt = tf.reduce_sum(p_zt.log_prob(z_t), axis=-1)\n",
    "        log_q_zt = tf.reduce_sum(q_zt.log_prob(z_t), axis=-1)\n",
    "        weights = log_p_zt + log_p_xt_given_zt - log_q_zt\n",
    "\n",
    "        if self._tilt:\n",
    "            ## Calculating the log_r values for the t and t+1 timesteps\n",
    "            prev_log_r = tf.cond(tf.greater(t > 0), lambda: self.tilt(\n",
    "                state.rnn_out, state.latent_encoded, targets), lambda: 0.)\n",
    "            log_r = tf.cond(tf.less(t + 1, self.max_len_seq), lambda: self.tilt(\n",
    "                rnn_out, latent_encoded, self.targets_ta.read(t + 1)), lambda: 0.)\n",
    "            log_r *= tf.to_float(t < self.seq_lengths - 1)\n",
    "            weights += log_r - prev_log_r\n",
    "\n",
    "        ## reshaping rnn_out so that it reports correctly - from the tensor array\n",
    "        rnn_out = tf.reshape(rnn_out, tf.shape(state.rnn_out))\n",
    "\n",
    "        ## Setting new state of the SRNN for all the 3 variables - i,h,c\n",
    "        new_state = TrainableSRNNState(\n",
    "            rnn_out=rnn_out, latent_encoded=latent_encoded, rnn_state=state.rnn_state)\n",
    "\n",
    "        return weights, new_state\n",
    "\n",
    "\n",
    "## Creating the method which does all the calls\n",
    "def create_stochastic_rnn(\n",
    "    data_size, latent_size, rnn_hidden_size=None,\n",
    "    fcnet_hidden_sizes=None, encoded_data_size=None, encoded_latent_size=None,\n",
    "    sigma_min=0.0, raw_sigma_bias=0.25, emission_bias_init=0.0, use_tilt=False,\n",
    "    proposal_type='filtering', random_seed=None):\n",
    "\n",
    "    ## Default initialization for the weights and the biases\n",
    "    INITIALIZERS = {\n",
    "        'w': tf.contrib.layers.xavier_initializer(),\n",
    "        'b': tf.zeros_initializer()\n",
    "    }\n",
    "\n",
    "    ## Set all values which are none to be latent_size if nothing given\n",
    "    if rnn_hidden_size is None:\n",
    "        rnn_hidden_size = latent_size\n",
    "    if fcnet_hidden_sizes is None:\n",
    "        fcnet_hidden_sizes = [latent_size]\n",
    "    if encoded_data_size is None:\n",
    "        encoded_data_size = latent_size\n",
    "    if encoded_latent_size is None:\n",
    "        encoded_latent_size = latent_size\n",
    "\n",
    "    ## Encode the data where the output_sizes are given as a list\n",
    "    data_encoder = snet.nets.MLP(\n",
    "        output_sizes=fcnet_hidden_sizes + [encoded_data_size],\n",
    "        initializers=INITIALIZERS,\n",
    "        name='data_encoder'\n",
    "    )\n",
    "    latent_encoder = snet.nets.MLP(\n",
    "        output_sizes=fcnet_hidden_sizes + [encoded_latent_size],\n",
    "        initializers=INITIALIZERS,\n",
    "        name='latent_encoder'\n",
    "    )\n",
    "    ## By applying Conditional Normal Distribution we are getting the latent\n",
    "    ## states by giving it the data\n",
    "    transition = ConditionalNormalDistribution(\n",
    "        size=latent_size,\n",
    "        hidden_layer_sizes=fcnet_hidden_sizes,\n",
    "        sigma_min=sigma_min,\n",
    "        raw_sigma_bias=raw_sigma_bias,\n",
    "        initializers=INITIALIZERS,\n",
    "        name='prior'\n",
    "    )\n",
    "    ## For getting the reconstructed output\n",
    "    emission = ConditionalNormalDistribution(\n",
    "        size=data_size,\n",
    "        hidden_layer_sizes=fcnet_hidden_sizes,\n",
    "        initializers=INITIALIZERS,\n",
    "        name='generative'\n",
    "    )\n",
    "\n",
    "    ## Instantiating \n",
    "    proposal = None\n",
    "    if proposal_type in ['filtering', 'smoothing']:\n",
    "        ## Approximating the output with the one from the latent space\n",
    "        proposal = NormalApproximatePosterior(\n",
    "            size=latent_size,\n",
    "            hidden_layer_sizes=fcnet_hidden_sizes,\n",
    "            initializers=INITIALIZERS,\n",
    "            smoothing=(proposal_type == 'smoothing'),\n",
    "            name='approximate_posterior'\n",
    "        )\n",
    "\n",
    "    ## Instantiating tilt with the normal distribution or None\n",
    "    tilt = None\n",
    "    if use_tilt:\n",
    "        tilt = ConditionalNormalDistribution(\n",
    "            size=data_size,\n",
    "            hidden_layer_sizes=fcnet_hidden_sizes,\n",
    "            initializers=INITIALIZERS,\n",
    "            name='tilt'\n",
    "        )\n",
    "\n",
    "    ## Instantiate rnn cell and reverse rnn cell\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(rnn_hidden_size, initializer=INITIALIZERS['w'])\n",
    "    rev_rnn_cell = tf.nn.rnn_cell.LSTMCell(rnn_hidden_size, initializer=INITIALIZERS['w'])\n",
    "\n",
    "    return TrainableStochasticRNN(\n",
    "        rnn_cell, data_encoder, latent_encoder,\n",
    "        transition, emission, proposal_type, proposal=proposal,\n",
    "        rev_rnn_cell=rev_rnn_cell, tilt=tilt, random_seed=random_seed\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdkit                     2018.09.2.0      py36h865188c_1    rdkit\r\n"
     ]
    }
   ],
   "source": [
    "!conda list | grep rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://gdbtools.unibe.ch:8080/cdn/gdb11.tgz ## Need to open a VPN connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "smifile = '/mnt/c/Users/harshdeep.harshdeep/AppData/Local/Packages/CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc/LocalState/gdb11_size08.smi'\n",
    "data = pd.read_csv(smifile, delimiter = \"\\t\", names = [\"smiles\",\"No\",\"Int\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>No</th>\n",
       "      <th>Int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CC(C)CC(C)(C)C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CC(N)CC(C)(C)C</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CC(O)CC(C)(C)C</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CC(F)CC(C)(C)C</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CC(C)(C)CC(F)F</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66701</td>\n",
       "      <td>C1C2C3C4CN(C14)C23</td>\n",
       "      <td>66702</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66702</td>\n",
       "      <td>C1C2C3CN4C2C4C13</td>\n",
       "      <td>66703</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66703</td>\n",
       "      <td>C1C2C3NC2C4C3N14</td>\n",
       "      <td>66704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66704</td>\n",
       "      <td>C1C2C3OC2C4C3N14</td>\n",
       "      <td>66705</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66705</td>\n",
       "      <td>C1C2C3CC2N4C1C34</td>\n",
       "      <td>66706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66706 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   smiles     No  Int\n",
       "0          CC(C)CC(C)(C)C      1    1\n",
       "1          CC(N)CC(C)(C)C      2    1\n",
       "2          CC(O)CC(C)(C)C      3    1\n",
       "3          CC(F)CC(C)(C)C      4    1\n",
       "4          CC(C)(C)CC(F)F      5    1\n",
       "...                   ...    ...  ...\n",
       "66701  C1C2C3C4CN(C14)C23  66702    1\n",
       "66702    C1C2C3CN4C2C4C13  66703    1\n",
       "66703    C1C2C3NC2C4C3N14  66704    1\n",
       "66704    C1C2C3OC2C4C3N14  66705    1\n",
       "66705    C1C2C3CC2N4C1C34  66706    1\n",
       "\n",
       "[66706 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the training, testing dataset, convert it into one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50029,) (16677,)\n"
     ]
    }
   ],
   "source": [
    "smiles_train, smiles_test = train_test_split(data[\"smiles\"], random_state=0)\n",
    "print(smiles_train.shape, smiles_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of the compounds :-> 23\n"
     ]
    }
   ],
   "source": [
    "print('Max length of the compounds :->', data.smiles.str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4', '+', 'n', '[', 'C', '(', '!', 'N', '-', '2', ')', '=', 'F', ']', 'E', 'c', 'H', 'O', '1', '3', 'o', '#'}\n",
      "22 24\n"
     ]
    }
   ],
   "source": [
    "charset = set(\"\".join(list(data.smiles)) + \"!E\") # Get all the unique characters\n",
    "char_to_int = dict((c,i) for i,c in enumerate(charset)) # Set unique indexes for each character\n",
    "int_to_char = dict((i,c) for i,c in enumerate(charset)) # vice versa as above\n",
    "embed = max([len(smile) for smile in data.smiles]) + 1 # Set max length which needs to be fed into the RNN  + 5 for some other\n",
    "print(str(charset))\n",
    "print(len(charset), embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(smiles):\n",
    "        one_hot =  np.zeros((smiles.shape[0], embed , len(charset)), dtype=np.int8)\n",
    "        for i,smile in enumerate(smiles):\n",
    "            #encode the startchar\n",
    "            one_hot[i, 0, char_to_int[\"!\"]] = 1\n",
    "            #encode the rest of the chars\n",
    "            for j,c in enumerate(smile):\n",
    "                one_hot[i, j + 1, char_to_int[c]] = 1\n",
    "            #Encode endchar\n",
    "            one_hot[i, len(smile) + 1:, char_to_int[\"E\"]] = 1\n",
    "        #Return two, one for input and the other for output\n",
    "        return one_hot[:, 0:-1, :], one_hot[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN1CC1C#CCO\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = vectorize(smiles_train.values)\n",
    "X_test, Y_test = vectorize(smiles_test.values)\n",
    "print(smiles_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_char[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_generator():\n",
    "    for i in range(len(X_train)):\n",
    "        yield X_train[i], Y_train[i], len(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = X_train[0].shape\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "      smiles_generator,\n",
    "      output_types=(tf.float32, tf.float32, tf.int32),\n",
    "      output_shapes=([n_rows, n_cols], [n_rows, n_cols], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.padded_batch(64, padded_shapes=([n_rows, n_cols], [n_rows, n_cols], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN 1: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "\n",
      "\n",
      " TEST 1: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for val in dataset.take(1):\n",
    "    print('TRAIN 1: {} \\n\\n\\n TEST 1: {}'.format(\n",
    "        val[0].numpy()[0], val[1].numpy()[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(X_train[0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model time - Stochastic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, Y_train.shape) ## Same because we are encoding and decoding for the same time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent space exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_latent = smiles_to_latent_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molno = 10\n",
    "latent_mol = smiles_to_latent_model.predict(X_test[molno:molno+1])\n",
    "sorti = np.argsort(np.sum(np.abs(x_latent - latent_mol), axis=1))\n",
    "print(sorti[0:10])\n",
    "print(smiles_test.iloc[sorti[0:8]])\n",
    "Draw.MolsToImage(smiles_test.iloc[sorti[0:8]].apply(Chem.MolFromSmiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Most different molecules in the molecule space\n",
    "print(smiles_test.iloc[sorti[-8:]])\n",
    "Draw.MolsToImage(smiles_test.iloc[sorti[-8:]].apply(Chem.MolFromSmiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_to_smiles(latent):\n",
    "    #decode states and set Reset the LSTM cells with them\n",
    "    states = latent_to_states_model.predict(latent)\n",
    "    sample_model.layers[1].reset_states(states=[states[0],states[1]])\n",
    "    #Prepare the input char\n",
    "    startidx = char_to_int[\"!\"]\n",
    "    samplevec = np.zeros((1,1,22))\n",
    "    samplevec[0,0,startidx] = 1\n",
    "    smiles = \"\"\n",
    "    #Loop and predict next char\n",
    "    for i in range(28):\n",
    "        o = sample_model.predict(samplevec)\n",
    "        sampleidx = np.argmax(o)\n",
    "        samplechar = int_to_char[sampleidx]\n",
    "        if samplechar != \"E\":\n",
    "            smiles = smiles + int_to_char[sampleidx]\n",
    "            samplevec = np.zeros((1,1,22))\n",
    "            samplevec[0,0,sampleidx] = 1\n",
    "        else:\n",
    "            break\n",
    "    return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolation test in latent_space\n",
    "i = 0\n",
    "j= 2\n",
    "latent1 = x_latent[j:j+1]\n",
    "latent0 = x_latent[i:i+1]\n",
    "mols1 = []\n",
    "print('Latent 0 : {} and Latent 1: {}'.format(latent_to_smiles(latent0), latent_to_smiles(latent1)))\n",
    "print('\\n\\n')\n",
    "\n",
    "ratios = np.linspace(0,1,25)\n",
    "for r in ratios:\n",
    "    #print r\n",
    "    # Using linear interpolation\n",
    "    rlatent = (1.0-r)*latent0 + r*latent1\n",
    "    smiles  = latent_to_smiles(rlatent)\n",
    "    # print(smiles)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        mols1.append(mol)\n",
    "    else:\n",
    "        print('NOT FOUND IN THE SMILES DATASET', smiles)\n",
    "# Draw.MolsToGridImage(mols1, molsPerRow=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp = smiles_test.apply(Chem.MolFromSmiles).apply(Descriptors.MolLogP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "red = pca.fit_transform(x_latent)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(red[:,0], red[:,1],marker='.', c= logp)\n",
    "print(pca.explained_variance_ratio_, np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# molmr = smiles_test.apply(Chem.MolFromSmiles).apply(Descriptors.MolMR)\n",
    "# plt.figure()\n",
    "# plt.scatter(red[:,0], red[:,1],marker='.', c= molwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptors.MolLogP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "wrong = 0\n",
    "for i in tqdm_notebook(range(len(x_latent))):\n",
    "    smiles = latent_to_smiles(x_latent[i:i+1])\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        print(smiles)\n",
    "        wrong = wrong + 1\n",
    "print(\"%0.1F percent wrongly formatted smiles\"%(wrong/float(1000)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
